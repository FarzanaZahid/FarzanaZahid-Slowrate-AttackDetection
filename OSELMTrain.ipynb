{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "import traceback\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from os_elm import OS_ELM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(label_classify):\n",
    "    c = np.max(label_classify, axis=-1).reshape(-1, 1)\n",
    "    exp_label_classify = np.exp(label_classify- c)\n",
    "    sum_exp_label_classify = np.sum(exp_label_classify, axis=-1).reshape(-1, 1)\n",
    "    return exp_label_classify/sum_exp_label_classify \n",
    "\n",
    "# Instantiate os-elm\n",
    "# ===========================================\n",
    "def initialiseOSELM(n_input_nodes, n_hidden_nodes, n_output_nodes):\n",
    "    print(\"initialiseOSELM ===========================================\")\n",
    "    try:\n",
    "        os_elm = OS_ELM(\n",
    "            # the number of input nodes.\n",
    "            n_input_nodes=n_input_nodes,\n",
    "            # the number of hidden nodes.\n",
    "            n_hidden_nodes=n_hidden_nodes,\n",
    "            # the number of output nodes.\n",
    "            n_output_nodes=n_output_nodes,\n",
    "                 \n",
    "            # NOTE: OS-ELM apply an activation function only to the hidden nodes.\n",
    "            activation='sigmoid',\n",
    "        )\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "    return os_elm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prapareData(x_classify, t_classify, n_classes, n_hidden_nodes):\n",
    "   \n",
    "    # dataset preparation\n",
    "    # ===========================================\n",
    "    print(\"prapareData ===========================================\")\n",
    "    print(f'x_classify sahpe : {x_classify.shape}')\n",
    "    print(f't_classify sahpe : {t_classify.shape}')\n",
    "\n",
    "    # shuffle dataset\n",
    "    perm = np.random.permutation(len(x_classify))\n",
    "    x_classify = x_classify[perm]\n",
    "    t_classify = t_classify[perm]\n",
    "\n",
    "    # divide dataset for training and testing\n",
    "    split_border = int(len(x_classify) * 0.98)\n",
    "\n",
    "    x_train, x_test = x_classify[:split_border], x_classify[split_border:]\n",
    "    t_train, t_test = t_classify[:split_border], t_classify[split_border:]\n",
    "    print(f'Training data from 0 - {split_border}')\n",
    "    print(f'Test data from {split_border} - {len(t_classify)}')\n",
    "\n",
    "    min_max_scalar = MinMaxScaler().fit(x_train)\n",
    "    x_train_normalised = min_max_scalar.transform(x_train)\n",
    "    x_test_normalised = min_max_scalar.transform(x_test)\n",
    "    print(f'x_train_normalised sahpe : {x_train_normalised.shape}')\n",
    "    print(f'x_test_normalised sahpe : {x_test_normalised.shape}')\n",
    "\n",
    "    t_test_categorised = to_categorical(t_test, num_classes=n_classes)\n",
    "    t_train_categorised = to_categorical(t_train, num_classes=n_classes)\n",
    "    print(f't_test_categorised sahpe : {t_test_categorised.shape}')\n",
    "    print(f't_train_categorised sahpe : {t_train_categorised.shape}')\n",
    "\n",
    "    # divide dataset for initial training\n",
    "    initial_train_border = int(n_hidden_nodes)\n",
    "    print(f'Initial Training data border from - {initial_train_border}')\n",
    "    x_trainclassify_init = x_train_normalised[:initial_train_border]\n",
    "    x_train_seq = x_train_normalised[initial_train_border:]\n",
    "    print(f'x_trainclassify_init sahpe : {x_trainclassify_init.shape}')\n",
    "    print(f'x_train_seq sahpe : {x_train_seq.shape}')\n",
    "\n",
    "    t_trainclassify_init = t_train_categorised[:initial_train_border]\n",
    "    t_train_seq = t_train_categorised[initial_train_border:]\n",
    "    print(f't_trainclassify_init sahpe : {t_trainclassify_init.shape}')\n",
    "    print(f't_train_seq sahpe : {t_train_seq.shape}')\n",
    "    return x_train_normalised, t_train_categorised, x_trainclassify_init, t_trainclassify_init, x_train_seq, t_train_seq, x_test_normalised, t_test_categorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(os_elm, x_train_normalised, x_trainclassify_init, t_trainclassify_init, x_train_seq, t_train_seq):\n",
    "    print(\"trainModel ===========================================\")\n",
    "    pbar = tqdm.tqdm(total=len(x_train_normalised), desc='initial training phase')\n",
    "    try:\n",
    "        # ===========================================\n",
    "        # Training\n",
    "        # ===========================================\n",
    "        # the initial training phase\n",
    "        try:\n",
    "            print(f'******************inital training Start************')\n",
    "            os_elm.init_train(x_trainclassify_init, t_trainclassify_init)\n",
    "            print(f'******************inital training Complete************')\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            pbar.update(n=len(x_trainclassify_init))\n",
    "            # the sequential training phase\n",
    "            \n",
    "            # ===========================================\n",
    "            # Training\n",
    "            # ===========================================\n",
    "            # the sequential training phase\n",
    "        pbar.set_description('Sequential training phase')\n",
    "        batch_size = 11\n",
    "        print(f'******************Sequential training Start************')\n",
    "        print(f'len(x_train_seq) :: {len(x_train_seq)}')\n",
    "        for i in range(0, len(x_train_seq), batch_size):\n",
    "            x_batch = x_train_seq[i:i+batch_size]\n",
    "            t_batch = t_train_seq[i:i+batch_size]\n",
    "            os_elm.seq_train(x_batch, t_batch)\n",
    "            pbar.update(n=len(x_batch))    \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "    finally:\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOnTestSample(os_elm, n, x_test_normalised, t_test_categorised):\n",
    "    print(\"predictOnTestSample ===========================================\")\n",
    "    # ===========================================\n",
    "    # Prediction\n",
    "    # ===========================================\n",
    "    # sample 10 validation samples from x_test\n",
    "    # n = 10\n",
    "    x = None\n",
    "    t = None\n",
    "    y = None\n",
    "    x = x_test_normalised[:n]\n",
    "    t = t_test_categorised[:n]\n",
    "\n",
    "    # 'predict' method returns raw values of output nodes.\n",
    "\n",
    "    try:\n",
    "        y = os_elm.predict(x)\n",
    "        # print(f'Y result{y}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "    # apply softmax function to the output values.\n",
    "    y = softmax(y)\n",
    "    # print(f'SOFTmax result{y}')\n",
    "\n",
    "    # check the answers.\n",
    "    max_ind_classify = None\n",
    "    for i in range(len(y-1)):\n",
    "    # for i in range(n):\n",
    "        max_ind_classify = np.argmax(y[i])\n",
    "        print('========== index number %d ==========' % i)\n",
    "        print('estimated class %d' % max_ind_classify)\n",
    "        print('estimated probability: %.3f' % y[i,max_ind_classify])\n",
    "        print('true answer: class %d' % np.argmax(t[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateTestData(os_elm, x_test_normalised, t_test_categorised, metrics=['ttime', 'accuracy']):\n",
    "    # ===========================================\n",
    "    # Evaluation\n",
    "\n",
    "    try:\n",
    "        [ttime, accuracy] = os_elm.evaluate(x_test_normalised, t_test_categorised, metrics=metrics)\n",
    "        print(f'time: {ttime} , accuracy: {accuracy}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "def saveModel(os_elm):\n",
    "    # ===========================================\n",
    "    # Save model\n",
    "    # ===========================================\n",
    "    print('saving model parameters...')\n",
    "    os_elm.save('./checkpoint/model.ckpt')\n",
    "    print('model parameters saved')\n",
    "\n",
    "    # initialize weights of os_elm\n",
    "    os_elm.initialize_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************\n",
    "# Initialise Variables\n",
    "# ****************************************\n",
    "initialise_os_elm = True\n",
    "train_model = True\n",
    "load_data = True\n",
    "n_input_nodes = 3\n",
    "n_hidden_nodes = 5\n",
    "n_output_nodes = 5\n",
    "n_classes = n_output_nodes\n",
    "start_data_limit = 0\n",
    "end_data_limit = 50000\n",
    "\n",
    "t_classify = None\n",
    "x_classify = None\n",
    "x_classify_df = None\n",
    "os_elm = None\n",
    "input_data = None\n",
    "pd.options.mode.chained_assignment = None\n",
    "input_Data_length = 0\n",
    "count = 1\n",
    "iteration_limit = 100\n",
    "n = 0\n",
    "while train_model:\n",
    "    print(f\"start_data_limit : {start_data_limit}, end_data_limit : {end_data_limit}\")\n",
    "    # # ===========================================\n",
    "    # # Load dataset\n",
    "    # # ===========================================\n",
    "    if initialise_os_elm:\n",
    "        os_elm = initialiseOSELM(n_input_nodes, n_hidden_nodes, n_output_nodes)\n",
    "        initialise_os_elm = False\n",
    "\n",
    "    # ===========================================\n",
    "    # Load dataset\n",
    "    # ===========================================\n",
    "    if load_data:\n",
    "        input_data = pd.read_csv(\"TrainingData20K.csv\")\n",
    "        input_Data_length = len(input_data.index)\n",
    "        print(f'Total records Loaded : {len(input_data.index)}')\n",
    "        load_data = False\n",
    "        \n",
    "        x_classify_df = input_data[['Tot_Fwd_Pkts','Fwd_Header_Len','Fwd_IAT_Max', 'Label']]\n",
    "\n",
    "        x_classify_df_with_label = x_classify_df\n",
    "   \n",
    "    x_classify_df = x_classify_df_with_label[['Tot_Fwd_Pkts','Fwd_Header_Len','Fwd_IAT_Max']]\n",
    "    x_classify=np.array(x_classify_df[start_data_limit:end_data_limit])\n",
    "    t_classify = x_classify_df_with_label['Label']\n",
    "    t_classify=np.array(t_classify[:len(x_classify)])\n",
    "\n",
    "    start_data_limit = end_data_limit\n",
    "    n = n+2;\n",
    "    end_data_limit = end_data_limit + (n + n_hidden_nodes)\n",
    "    \n",
    "    # ===========================================\n",
    "    # Prepare dataset\n",
    "    # ===========================================\n",
    "    x_train_normalised, t_train_categorised, x_trainclassify_init, t_trainclassify_init, x_train_seq, t_train_seq, x_test_normalised, t_test_categorised = prapareData(x_classify, t_classify, n_classes, n_hidden_nodes)\n",
    "    # ===========================================\n",
    "    # Training Model\n",
    "    # ===========================================\n",
    "    trainModel(os_elm, x_train_normalised, x_trainclassify_init, t_trainclassify_init, x_train_seq, t_train_seq)\n",
    "    # ===========================================\n",
    "    # Prediction Sample 10 records\n",
    "    # ===========================================\n",
    "    # predictOnTestSample(os_elm, 10, x_test_normalised, t_test_categorised)\n",
    "    # ===========================================\n",
    "    # Evaluation\n",
    "    # ===========================================\n",
    "    evaluateTestData(os_elm, x_test_normalised, t_test_categorised, metrics=['ttime', 'accuracy'])\n",
    "    # ===========================================\n",
    "    # Save model\n",
    "    # ===========================================\n",
    "    saveModel(os_elm)\n",
    "\n",
    "    count = count +1\n",
    "    if end_data_limit > input_Data_length or count > iteration_limit:\n",
    "        print(f\"end_data_limit : {end_data_limit}, input_Data_length : {input_Data_length}\")\n",
    "        train_model = False\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
